///! Optimizations for the MIR
use calamars_core::Identifier;
use hashbrown::{HashMap, HashSet};

use crate::{
    BBlock, BlockId, Function, VInstructionKind, ValueId, errors::MirErrors, lower::MirRes,
};

/// An optimization pass on some function
pub trait OptFunction {
    type PassContext: Into<OptLevel>;
    type PassOutput;

    /// Name
    fn name(&self) -> &'static str;

    /// Description
    fn desc(&self) -> &'static str;

    /// Run optimization on the inputted function
    fn optimize(&mut self, f: &mut Function, ctx: Self::PassContext) -> MirRes<Self::PassOutput>;
}

#[derive(PartialEq, Eq)]
pub enum OptLevel {
    None,
    All,
}

impl Into<OptLevel> for usize {
    fn into(self) -> OptLevel {
        match self {
            0 => OptLevel::None,
            _ => OptLevel::All,
        }
    }
}

pub struct TailCallOptimization {
    /// how many times was a valueid used
    usages: HashMap<ValueId, usize>,
}

impl TailCallOptimization {
    pub fn new() -> Self {
        Self {
            usages: HashMap::new(),
        }
    }

    fn insert_usage(&mut self, vid: &ValueId) {
        if let Some(usagec) = self.usages.get_mut(vid) {
            *usagec += 1;
        } else {
            self.usages.insert(*vid, 1);
        }
    }

    fn fill_usage_with_instructions(&mut self, f: &Function) {
        for vinst in &f.instructions {
            match &vinst.kind {
                VInstructionKind::Binary { lhs, rhs, .. }
                | VInstructionKind::BitwiseBinary { lhs, rhs, .. } => {
                    self.insert_usage(lhs);
                    self.insert_usage(rhs);
                }
                VInstructionKind::Unary { on, .. } => self.insert_usage(on),
                VInstructionKind::Call { args, .. } => {
                    args.iter().for_each(|vid| self.insert_usage(vid))
                }
                _ => {}
            };
        }
    }

    fn fill_usage_with_terminators(&mut self, blocks: &[BBlock]) {
        for block in blocks {
            if let Some(term) = &block.finally {
                match term {
                    crate::Terminator::Return(Some(vid))
                    | crate::Terminator::BrIf { condition: vid, .. } => self.insert_usage(vid),
                    crate::Terminator::Call { args, .. } => {
                        args.iter().for_each(|vid| self.insert_usage(vid))
                    }
                    _ => {}
                }
            }
        }
    }

    fn fill_usages(&mut self, f: &Function) {
        self.fill_usage_with_instructions(f);
        self.fill_usage_with_terminators(&f.blocks);
    }

    /// Return blocks by id that need to be tail call optimized
    fn blocks_to_update(&mut self, f: &Function) -> Vec<usize> {
        let mut block_ids = vec![];
        for (id, block) in f.blocks.iter().enumerate() {
            if let Some(crate::Terminator::Return(Some(v))) = &block.finally {
                // check if the value id came from a function call
                let inst = f.instructions.get(v.inner_id()).unwrap();
                if matches!(&inst.kind, VInstructionKind::Call { .. })
					// It was only used in the return statement and exactly once
					&& self.usages[v] == 1
				    // It was the last thing done in the block (this is important due to the possibility of side effects)
					&& block.instructs.last().is_some_and(|x| x == v)
                {
                    block_ids.push(id)
                }
            }
        }
        block_ids
    }

    // This is a little iffy - If the terminator fails, we have already popped the instruction so we will have
    // invalid MIR. Oh well, this is a pre alpha mainly for testing, so we will allow it for now, and add a TODO.
    fn update_block(&mut self, f: &mut Function, blockid: usize) -> MirRes<()> {
        let block = &mut f.blocks[blockid];
        let last = block
            .instructs
            .pop()
            .ok_or(MirErrors::InstructionListWasEmpty)?;
        let terminator = f
            .instructions
            .get(last.inner_id())
            .ok_or(MirErrors::InstNotFound)?
            .kind
            .call_to_terminator()?;
        block.finally = Some(terminator);
        Ok(())
    }
}

impl OptFunction for TailCallOptimization {
    type PassContext = usize;
    type PassOutput = ();

    fn name(&self) -> &'static str {
        "Tail call optimization"
    }

    fn desc(&self) -> &'static str {
        "If the terminator instruction points to the data generated by a function call, we can tail call optimize."
    }

    fn optimize(&mut self, f: &mut Function, _ctx: Self::PassContext) -> MirRes<Self::PassOutput> {
        self.fill_usages(f);
        let to_update = self.blocks_to_update(&f);
        for block_id in to_update {
            self.update_block(f, block_id)?;
        }
        Ok(())
    }
}

/// When returning an if statement, we don't need to return the phi value
/// generated, we can just move the return into the function body.
///
/// I.e.
/// ```text
/// bb0:
///   %v0 = param #0
///   %v1 = param #1
///   %v2 = %v0 == %v1
///   br_if %v2, then: bb1 else: bb2
/// bb1:
///   %v3 = const true
///   br bb3
/// bb2:
///   %v4 = const false
///   br bb3
/// bb3:
///   %v5 = phi ty#4 [bb1: %v3, bb2: %v4]
///   return %v5
/// ```
///
/// Should be optimized to
/// ```text
/// bb0:
///   %v0 = param #0
///   %v1 = param #1
///   %v2 = %v0 == %v1
///   br_if %v2, then: bb1 else: bb2
/// bb1:
///   %v3 = const true
///   return %v3
/// bb2:
///   %v4 = const false
///   return %v4
/// ```
///
/// This allows for better tail call optimization. Note that this optimization
/// needs to be run many times for the same function, to handle nested ifs.
pub struct PhiReturnOptimization {
    // We need a whitelist since we don't delete the block in which the phi is resolved.
    // Otherwise, we would end up in infinite loops. Once we find a way to delete blocks
    // we can safely remove this whitelist.
    whitelist: HashSet<ValueId>,
}

impl PhiReturnOptimization {
    pub fn new() -> Self {
        Self {
            whitelist: HashSet::new(),
        }
    }

    /// Find which blocks we can optimize
    fn blocks_to_optimize(&self, f: &Function) -> Vec<(BlockId, ValueId)> {
        let mut blocks = vec![];
        for (blockid, block) in f.blocks.iter().enumerate() {
            if let Some(crate::Terminator::Return(Some(vid))) = block.finally {
                // check if the value id was from a phi block
                let ik = f.instructions.get(vid.inner_id()).map(|x| &x.kind);
                if matches!(ik, Some(VInstructionKind::Phi { .. }))
                    && !self.whitelist.contains(&vid)
                {
                    let blockid = BlockId::from(blockid);
                    blocks.push((blockid, vid))
                }
            }
        }
        blocks
    }

    /// Given one block, we can now optimize it
    fn optimize_block(
        &mut self,
        f: &mut Function,
        phi_resolution_block: BlockId,
        value_id: ValueId,
    ) {
        let inst = &f
            .instructions
            .get(value_id.inner_id())
            // This is safe since we make sure it exists when generating the data
            .unwrap()
            .kind;

        let VInstructionKind::Phi { incoming, .. } = inst else {
            unreachable!("optimize_block called with non-phi");
        };

        for (incoming_block_id, incoming_value_id) in incoming {
            let block = f.blocks.get_mut(incoming_block_id.inner_id()).unwrap();
            // Lets make sure that this optimization is safe to do for this specific block
            // It should always be the cases, buy maybe another optimization later will have
            // played with this block already ...
            if let Some(crate::Terminator::Br { target }) = &block.finally
                && *target == phi_resolution_block
            {
                block.finally = Some(crate::Terminator::Return(Some(*incoming_value_id)));
            }
        }

        // FIXME: We should delete the block, but right now it messes with the indices ...
        // for now we will leave it as is, it should be an unreachable block.

        self.whitelist.insert(value_id);
    }
}

impl OptFunction for PhiReturnOptimization {
    type PassContext = usize;

    type PassOutput = ();

    fn name(&self) -> &'static str {
        "Phi return optimization"
    }

    fn desc(&self) -> &'static str {
        "When returning an ifs value, move the return into the bodies of the statements"
    }

    fn optimize(&mut self, f: &mut Function, ctx: Self::PassContext) -> MirRes<Self::PassOutput> {
        if Into::<OptLevel>::into(ctx) != OptLevel::All {
            return Ok(());
        }

        loop {
            let opt_spots = self.blocks_to_optimize(&f);
            if opt_spots.is_empty() {
                break;
            }

            for (b, v) in opt_spots {
                self.optimize_block(f, b, v);
            }
        }

        return Ok(());
    }
}
